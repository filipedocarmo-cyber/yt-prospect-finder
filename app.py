"""
YT Prospect Finder â€” Canais Pequenos com VÃ­deos Virais (Streamlit)
-----------------------------------------------------------------
- Agora com **seÃ§Ã£o Em Alta** na tela principal:
  - Canais **â‰¤ 20.000 inscritos**
  - VÃ­deos **â‰¥ 20 minutos**
  - **â‰¥ 10.000 views nos Ãºltimos 7 dias** (proxy: vÃ­deos publicados nos Ãºltimos 7 dias com views totais â‰¥ 10k)
- MantÃ©m filtros gerais personalizÃ¡veis na barra lateral.

Obs.: A API pÃºblica do YouTube nÃ£o retorna views por janela de 7 dias por vÃ­deo (isso Ã© do
YouTube Analytics e requer OAuth). Usamos um **proxy confiÃ¡vel**: vÃ­deos publicados
nos **Ãºltimos 7 dias** com **views totais â‰¥ 10k** e que pertenÃ§am a **canais â‰¤ 20k inscritos**.
"""

from datetime import datetime, timedelta
from typing import List, Any
import pandas as pd
import streamlit as st
from googleapiclient.discovery import build
import isodate

st.set_page_config(page_title="YT Prospect Finder â€” Canais Pequenos com VÃ­deos Virais", layout="wide")
st.title("ðŸ”Ž YT Prospect Finder â€” Canais Pequenos com VÃ­deos Virais")

with st.sidebar:
    st.header("ConfiguraÃ§Ãµes")
    api_key = st.text_input("YouTube API Key", type="password")
    raw_queries = st.text_area(
        "Palavrasâ€‘chave",
        "historias emocionantes, desaparecidos, padre cÃ­cero",
        help="Separe por vÃ­rgula. Vamos buscar por views em cada termo.",
    )
    region = st.selectbox("RegiÃ£o", ["BR","US","MX","ES","FR","PL","IT","PT","AR","CO","CL"], index=0)
    published_after = st.date_input("Publicado depois de", (datetime.utcnow()-timedelta(days=365)).date())

    # Filtros gerais (para tabela padrÃ£o)
    min_views_general = st.select_slider("MÃ­nimo de views (tabela geral)", options=[10_000,50_000,100_000,200_000,500_000,1_000_000], value=200_000)
    max_subs_general = st.number_input("MÃ¡x. inscritos (tabela geral)",1,200_000,10_000,500)
    min_duration_general = st.number_input("â±ï¸ DuraÃ§Ã£o mÃ­nima (min) â€” geral", min_value=0, max_value=180, value=10, step=1)

    max_per_query = st.slider("MÃ¡x. vÃ­deos por palavraâ€‘chave",20,200,100,20)

    st.markdown("---")
    st.caption("A seÃ§Ã£o **Em Alta** usa critÃ©rios fixos: â‰¤20k inscritos, â‰¥20min, â‰¥10k views e publicaÃ§Ã£o nos Ãºltimos 7 dias.")

@st.cache_data(show_spinner=False)
def yt_client(api_key:str):
    return build("youtube","v3",developerKey=api_key)

def chunked(lst:List[str], size:int)->List[List[str]]:
    return [lst[i:i+size] for i in range(0,len(lst),size)]

def safe_int(x:Any,default:int=0)->int:
    try: return int(x)
    except: return default

def parse_duration_minutes(duration_str:str)->float:
    try:
        td=isodate.parse_duration(duration_str)
        return td.total_seconds()/60
    except: return 0.0

def search_videos(service,query,region,published_after_iso,limit:int)->List[str]:
    video_ids=[]; page_token=None
    while len(video_ids)<limit:
        res=service.search().list(
            part="id",
            type="video",
            order="viewCount",
            q=query,
            regionCode=region,
            publishedAfter=published_after_iso,
            maxResults=min(50,limit-len(video_ids)),
            pageToken=page_token,
            safeSearch="none"
        ).execute()
        for item in res.get("items",[]):
            vid=item["id"].get("videoId")
            if vid: video_ids.append(vid)
        page_token=res.get("nextPageToken")
        if not page_token: break
    return video_ids

def get_videos_stats(service,video_ids:List[str])->pd.DataFrame:
    rows=[]
    for ids_batch in chunked(video_ids,50):
        res=service.videos().list(part="snippet,statistics,contentDetails",id=",".join(ids_batch),maxResults=50).execute()
        for it in res.get("items",[]):
            sn=it.get("snippet",{}); stt=it.get("statistics",{}); cd=it.get("contentDetails",{})
            rows.append({
                "videoId":it.get("id"),
                "title":sn.get("title"),
                "publishedAt":sn.get("publishedAt"),
                "channelId":sn.get("channelId"),
                "channelTitle_video":sn.get("channelTitle"),
                "views":safe_int(stt.get("viewCount")),
                "likes":safe_int(stt.get("likeCount")),
                "comments":safe_int(stt.get("commentCount")),
                "duration_min":parse_duration_minutes(cd.get("duration","PT0M"))
            })
    return pd.DataFrame(rows)

def get_channels_stats(service,channel_ids:List[str])->pd.DataFrame:
    rows=[]
    for ids_batch in chunked(channel_ids,50):
        res=service.channels().list(part="snippet,statistics",id=",".join(ids_batch),maxResults=50).execute()
        for it in res.get("items",[]):
            sn=it.get("snippet",{}); stt=it.get("statistics",{})
            rows.append({
                "channelId":it.get("id"),
                "channelTitle_channel":sn.get("title"),
                "subs":safe_int(stt.get("subscriberCount"),-1),
                "country":sn.get("country")
            })
    return pd.DataFrame(rows)

def build_links(df:pd.DataFrame)->pd.DataFrame:
    df=df.copy()
    df["videoUrl"]=df["videoId"].apply(lambda x:f"https://www.youtube.com/watch?v={x}")
    df["channelUrl"]=df["channelId"].apply(lambda x:f"https://www.youtube.com/channel/{x}")
    return df

# ====================== MAIN ======================
if st.button("ðŸš€ Buscar canais agora",type="primary"):
    if not api_key:
        st.error("Informe sua YouTube API Key."); st.stop()

    service=yt_client(api_key)
    queries=[q.strip() for q in raw_queries.split(",") if q.strip()]
    published_after_iso=datetime.combine(published_after,datetime.min.time()).isoformat("T")+"Z"

    # 1) Buscar vÃ­deos por termos
    all_video_ids=[]; pb=st.progress(0.0,text="Buscando vÃ­deosâ€¦")
    for i,q in enumerate(queries,start=1):
        vids=search_videos(service,q,region,published_after_iso,max_per_query)
        all_video_ids.extend(vids)
        pb.progress(i/len(queries),text=f"{q}: {len(vids)} vÃ­deos")

    videos_df=get_videos_stats(service,all_video_ids)
    if videos_df.empty:
        st.warning("Nenhum vÃ­deo encontrado."); st.stop()

    # 2) Trazer dados dos canais e unificar dado
    unique_channels=sorted(videos_df["channelId"].dropna().unique().tolist())
    ch_df=get_channels_stats(service,unique_channels)

    merged=videos_df.merge(ch_df,on=["channelId"],how="left",suffixes=("_video","_channel"))
    merged["channelTitle"]=merged.get("channelTitle_channel").fillna(merged.get("channelTitle_video"))
    merged=build_links(merged)
    merged["publishedAt"]=pd.to_datetime(merged["publishedAt"],errors="coerce")

    # =================== SeÃ§Ã£o EM ALTA (topo) ===================
    NOW_UTC = datetime.utcnow()
    cutoff_7d = NOW_UTC - timedelta(days=7)

    trending = merged[
        (merged["subs"] >= 0) & (merged["subs"] <= 20_000) &
        (merged["duration_min"] >= 20) &
        (merged["views"] >= 10_000) &
        (merged["publishedAt"] >= cutoff_7d)
    ].copy()

    trending = trending.sort_values(["views","publishedAt"], ascending=[False, False])

    st.subheader("ðŸ”¥ Em Alta (Ãºltimos 7 dias)")
    st.caption("Canais â‰¤ 20k inscritos â€¢ VÃ­deos â‰¥ 20 min â€¢ â‰¥ 10k views â€¢ Publicados nos Ãºltimos 7 dias")

    if trending.empty:
        st.info("Nenhum vÃ­deo em alta dentro desses critÃ©rios nesta busca. Tente outros termos/regiÃµes.")
    else:
        cols_trend = [
            "title","views","duration_min","publishedAt","channelTitle","subs","videoUrl","channelUrl"
        ]
        cols_trend = [c for c in cols_trend if c in trending.columns]
        st.dataframe(trending[cols_trend], use_container_width=True)

    st.markdown("---")

    # =================== Tabela geral (com filtros do usuÃ¡rio) ===================
    general = merged.copy()

    # Aplicar filtros gerais
    general = general[(general["views"] >= min_views_general) & (general["duration_min"] >= min_duration_general)]
    general = general[(general["subs"] >= 0) & (general["subs"] <= max_subs_general)]

    if general.empty:
        st.warning("Nenhum vÃ­deo atende aos filtros gerais definidos na barra lateral.")
        st.stop()

    general = general.sort_values(["views","publishedAt"], ascending=[False, False])

    st.subheader("ðŸ“‹ VÃ­deos Encontrados (tabela geral)")
    cols_gen = [
        "title","views","duration_min","publishedAt","channelTitle","subs","videoUrl","channelUrl"
    ]
    cols_gen = [c for c in cols_gen if c in general.columns]
    st.dataframe(general[cols_gen], use_container_width=True)

    # Downloads
    ts = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    st.download_button("â¬‡ï¸ CSV â€” Em Alta (7d)", data=trending[cols_trend].to_csv(index=False) if not trending.empty else "",
                       file_name=f"yt_trending_7d_{ts}.csv", mime="text/csv", disabled=trending.empty)
    st.download_button("â¬‡ï¸ CSV â€” Tabela Geral", data=general[cols_gen].to_csv(index=False),
                       file_name=f"yt_general_{ts}.csv", mime="text/csv")
else:
    st.info("Preencha a chave, defina suas palavrasâ€‘chave e clique em **Buscar canais agora**.")
